Understanding of Quantum Mechanics can be derived from tools of elementary linear algebra.

Linear algebra uses the notion of vector spaces, and the arena/space most useful for Quantum Mechanics is [latex]C^n[/latex], a space of all n- tuples (n dimensional vectors) of complex numbers, ([latex]z_{1}[/latex], [latex]z_{2}[/latex], . . . , [latex]z_{n}[/latex]). The zero vector, (0, 0, . . . , 0) is included in this space. Note, vector spaces are closed under scalar multiplication and addition.

Additional note: |[latex]v_{1}[/latex]> is Quantum Mechanics' notation for a vector, where [latex]v_{i}[/latex] is an arbitrary label.

Bases and linear independence

Each vector space V has a set of vectors |[latex]v_{1}[/latex]>, . . . , |[latex]v_{n}[/latex]> known as the spanning set or the basis of V, where each vector |[latex]v[/latex]> ϵ V is a linear combination of the elements in the spanning set.

|[latex]v[/latex]> = |[latex]v_{1}[/latex]> + . . . + |[latex]v_{n}[/latex]> = [latex]\Sigma_{i}[/latex][latex]a_{i}[/latex]|[latex]v_{i}[/latex]>,

where {[latex]a_{i}[/latex]} is the set of associated complex coefficients per vector |[latex]v_{i}[/latex]>. The primary condition for defining a basis (spanning set) is that |[latex]v_{1}[/latex]>, . . . , |[latex]v_{n}[/latex]> are not linearly dependent. Linear dependent means that there exists a set of complex numbers {[latex]a_{i}[/latex]}, with [latex]a_{i}[/latex] [latex]\neq[/latex] 0 for at least one value of i such that

[latex]\Sigma_{i}[/latex][latex]a_{i}[/latex]|[latex]v_{i}[/latex]> = 0

Linear operators and matrices

A linear operator F is defined to be a function F : V -> W, where F is linear in its input

F ([latex]\Sigma_{i}[/latex][latex]a_{i}[/latex]|[latex]v_{i}[/latex]>) = [latex]\Sigma_{i}[/latex][latex]a_{i}[/latex]F(|[latex]v_{i}[/latex]>)

V and W are vector spaces in the above operation.

A more convenience representation of a linear operator is in its matrix representation. Here's the connection: suppose you're given an m x n complex matrix F. Each entry is [latex]F_{ij}[/latex]. Under matrix multiplication, F simply sends each vector from the [latex]C^n[/latex] space to the [latex]C^m[/latex] space. Now, the below equation of the matrix F acting on [latex]\Sigma_{i}[/latex][latex]a_{i}[/latex] is true, given that this is the behavior of matrix multiplication.

F ([latex]\Sigma_{i}[/latex][latex]a_{i}[/latex]|[latex]v_{i}[/latex]>) = [latex]\Sigma_{i}[/latex][latex]a_{i}[/latex]F(|[latex]v_{i}[/latex]>)

Hence, the matrix F is a linear operator as well.

Pauli matrices

Here are 4 important 2 x 2 matrices, known as Pauli matirces, that are used extensively in the study of Quantum Mechanics

[latex]\sigma_{0}[/latex] = [latex]\mathbb{I}[/latex] = [latex]\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}[/latex]         [latex]\sigma_{1}[/latex] = [latex]\sigma_{x}[/latex] = X = [latex]\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}[/latex]

[latex]\sigma_{2}[/latex] = [latex]\sigma_{y}[/latex] = Y = [latex]\begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix}[/latex]        [latex]\sigma_{3}[/latex] = [latex]\sigma_{z}[/latex] = Z = [latex]\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}[/latex]

Inner products

Another significant operator of linear algebra is the inner product. It happens to be linear operator that takes two vectors (|[latex]v[/latex]> , |[latex]w[/latex]>) from a vector space V, and output a complex number c. In mathematical notation, this is defined as

<[latex]v[/latex]|[latex]w[/latex]> = c

Inner products must satisfy these properties:

(1) ( . , . ) is linear in the second argument

[latex]<v[/latex]|([latex]\Sigma_{i}\lambda_{i}|w_{i}>[/latex]) = [latex]\Sigma_{i}\lambda_{i}[/latex][latex]<v[/latex]|[latex]w_{i}>[/latex]

(2) [latex]<v[/latex]|[latex]w>[/latex] = [latex]<v[/latex]|[latex]w>^{*}[/latex]

(3) ([latex]<v[/latex]|[latex]v>[/latex])   [latex]\geq[/latex] 0 with equality if and only if |[latex]v>[/latex] = 0

Any vector space that satisfies the above properties when testing for inner products is defined as an inner product space.

In Quantum Mechanics, the finite dimensional complex inner product spaces are referred to as the Hilbert spaces. These spaces are usually described with orthonormal set of vectors {[latex]|v>[/latex]}.

Thus, suppose a set of vectors |[latex]w_{1}[/latex]>, . . . , |[latex]w_{n}[/latex]> is given as a basis set for a vector space V. The orthonormal basis |[latex]v_{1}[/latex]>, . . . , |[latex]v_{n}[/latex]> may be derived using what's called the Gram-Schmidt procedure.

Aside, another notation of use for linear operators is the outer product representation, defined as

[latex]|w>[/latex][latex]<v|[/latex],

where [latex]|v>[/latex] is a vector from a vector space V and [latex]|w>[/latex] is a vector from a vector space W, and the outer product takes an input vector from V and produces an output vector in W, as shown below

([latex]|w>[/latex][latex]<v|[/latex]) ([latex]|v'>[/latex]) = [latex]|w><v|v'>[/latex] = [latex]<v|v'>|w>[/latex]

One usefulness of the outer product representation it that it helps explain the completeness relation of orthonormal vectors [latex]|i>[/latex] that form a basis for a vector space V.

[latex]\Sigma_{i}|i><i| = \mathbb{I}[/latex]    (Completeness relation)

Eigenvectors and eigenvalues

Eigenvectors, of a linear operator F, are defined as non-zero vectors [latex]|v>[/latex] such that

[latex]F|v> = v|v>[/latex],

where [latex]v[/latex] is a complex number defined as the eigenvalue of F that's associated with [latex]|v>[/latex].

The tool for determining eigenvalues is the characteristic function (click on to get instruction on deriving eigenvalues). Note, the set of eigenvectors associated with an eigenvalue [latex]v[/latex] forms an eigenspace. When an eigenspace has more than one element, we say that it is n-dimensional, given the number of elements in the space, and that the space is degenerate, because the vectors in the space are linearly independent, yet have the same eigenvalue.

Adjoints and Hermatian operators

Given a linear operator F on a vector space V, there exists a linear operator called the adjoint (Hermatian conjugate of operator F), and is defined as [latex]F^{\dag}[/latex], where

[latex]<v|F|w> = F^{\dag}<v|w>[/latex] ([latex]|v>[/latex], [latex]|w>[/latex] ϵ V).

From this definition, we have that

[latex](FG)^{\dag} = G^{\dag}F^{\dag}[/latex]

and

[latex]|v>^{\dag}[/latex] = [latex]<v|[/latex].

In matrix representation of operator F, the adjoint acting on F gives the conjugate-transpose of the matrix representation for F. For example,

[latex]\begin{bmatrix} 2 + 6i & -3i \\ 3 + i & 2 - 5i \end{bmatrix}^{\dag}[/latex] = [latex]\begin{bmatrix} 2 - 6i & 3 - i \\ 3i & 2 + 5i \end{bmatrix}[/latex]

If

[latex]F = F^{\dag}[/latex],

then F is Hermatian.

One important class of Hermatian operators is defined as the projectors. Projectors P are defined as linear operators that act on a k-dimensional subspace W of V, where [latex]|i>[/latex] is a orthonormal basis and

[latex]P = \Sigma_{i=1}^{k}|i><i|[/latex].

As stated, P is Hermatian,

[latex]P = P^{\dag}[/latex].

Alongside, operator F is said to be normal if

[latex]FF^{\dag} = F^{\dag}F[/latex].

Thus, Hermatian operators are normal as well.

Aside, representation theorem spectral decomposition for normal operators state that operators are normal if and only if they are diagnolizable.

Last, for any matrix U, where

[latex]UU^{\dag} = U^{\dag}U = \mathbb{I}[/latex]

is true, U is then defined as unitary. Same applies to operators U, which, by derivation, means that U is normal and diagonalizable.

Unitary operators play an important geometric property on inner product spaces. Suppose [latex]|v>[/latex] and [latex]|w>[/latex] are vectors of inner product space V, then we have that

[latex]<v|U^{\dag}U|w> = <v|\mathbb{I}|w> = <v|w>[/latex].

Tensor products

Larger vector spaces can be formed by putting together smaller vector spaces through the technique of tensor products.

Let V and W be vector spaces of dimension m and n respectively, where |v> are elements of V and |w> are elements of W. A larger vector space V[latex]\oplus[/latex]W is created through the tensor product and each element is formed through a linear combination of tensor products of elements |v>[latex]\oplus[/latex]|w>. Note, an orthonormal basis |i>[latex]\oplus[/latex]|j>, where |i> is the orthonormal basis for V and |j> is the orthonormal basis for W, may be created for V[latex]\oplus[/latex]W.

The following properties also follow for tensor products:

For any scalar c, elements |v> of V, and |w> of W,
c(|v>[latex]\oplus[/latex]|w>) = (c|v>)[latex]\oplus[/latex]|w> = |v>[latex]\oplus[/latex](c|w>)

     2.  For any [latex]|v_{1}>[/latex] and [latex]|v_{2}>[/latex] in V and |w> in W,

([latex]|v_{1}> + |v_{2}>) \oplus |w>[/latex] = [latex]|v_{1}> \oplus |w> + |v_{2}> \oplus |w>[/latex]

     3. For any|v> in V and [latex]|w_{1}>[/latex] and [latex]|w_{2}>[/latex] in W,

[latex]|v> \oplus (|w_{1}> + |w_{2}>[/latex]) = [latex]|v> \oplus |w_{1}> + |v> \oplus |w_{2}>[/latex]

Now, operators that are linear when acting on |v>[latex]\oplus[/latex]|w> are defined as

(A[latex]\oplus[/latex]B)(|v>[latex]\oplus[/latex]|w>) = A|v>[latex]\oplus[/latex]B|w>,

where |v> [latex]\epsilon[/latex] V and |w> [latex]\epsilon[/latex] W, and A is a linear operator on V and B is a linear operator on W.

Furthermore, the inner products on the spaces V and W can be used to define the inner product on V[latex]\oplus[/latex]W:

([latex]\Sigma_{i}a_{i}|v_{i}> \oplus |w_{i}>[/latex], [latex]\Sigma_{j}b_{j}|v'_{j}> \oplus |w'_{j}>[/latex]) =  [latex]\Sigma_{ij}a_{i}^{*}b_{j}<v_{i}|v'_{j}><w_{i}|w'_{j}>[/latex]

From this follows that the inner product space of the tensor product is adjoint, unitary, normal, and Hermitian.

Now we'll to matrix representation, defined as Kronecker product. Let V be an m x n matrix, and B an p x q matrix. Then,

V[latex]\oplus[/latex]W = [latex]\begin{bmatrix} V_{11}W & V_{12}W & \dots & V_{1n}W \\ V_{21}W & V_{22}W & \dots & V_{2n}W \\ \vdots & \vdots & \vdots & \vdots \\ V_{m1}W & V_{m2}W & \dots & V_{mn}W \end{bmatrix}[/latex]

The matrix representation is a nq x mp matrix.

The commutator and anti-commutator

Commutator of two operators, A and B, is defined as

[A, B] = AB - BA

and the anti-commutator of A and B is defined as

{A,  B} = AB +BA.

In the former case, if [A, B] = 0, then A and B commute. In the latter case, if {A, B} = 0, the A and B anti-commute.

Many properties for pairs of operators are deduced from their commutation and anti-commutation. The property of simultaneously 'diagonalizing' Hermation operators A = [latex]\Sigma_{i}a_{i}\i><i|[/latex] and B = [latex]\Sigma_{i}b_{i}\i><i|[/latex], where |i> are some common orthonormal basis. The proper definition for this is stated below:

Simultaneous Diagonalization Theorem: Let A and B be Hermatian operators. [A, B] = 0 if and only if there exists a common orthonormal bases for which both A and B are diagonalized with respect to that basis.

Polar and singular value decompositions

Polar and singular value decompositions are techniques for breaking up linear operators into products of unitary and positive operators. Given that many linear operators are hard to understand, there's generally greater understanding for unitary and positive operators.

Polar Decomposition Theorem: Let F be a linear operator that acts on a vector space V. There then exists a unitary operator U and positive operator P and K such that

F = UP = KU,

where [latex]\sqrt{A^{\dag}A}[/latex] and [latex]\sqrt{AA^{\dag}}[/latex]. Alongside, if F is invertible, U is unique.

Singular Value Decomposition Corollary: If F is a square matrix, then there exists unitary matrices U and V, and diagonal matrix D with non-negative entries such that

F = UDV.

 