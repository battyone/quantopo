{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on Quantopian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, the Skikit-Learn package provides tools of Machine Learning for performing learning algorithms on imported data. Currently, Machine Learning divides into two camps: unsupervised learning and supervised learning. We'll focus on supervised learning here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "In supervised learning, users teach machines how to learn. Users provide a volume of data marked by their features and labels to train the machine with (in this case, the machine is essentially an algorithm (i.e. - classification algorithm) in a program). If the training performance is acceptable, according to the user, then subsequently, a separte set of similar data marked with just their features (labels are hidden from the machine and not the user) is sent to the machine to make predictions on the labels of this set of data and measure its accuracy. \n",
    "\n",
    "Below, is a simple example coded on Quantopian using SkiKit-Learn module on stock price data. The features are pricing movements and the labels are their future outcomes (\"up\" or \"down\"). \n",
    "\n",
    "Lets first focus on the initialize() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def initialize(context):\n",
    "\n",
    "    context.stocks = symbols('XLY',  # XLY Consumer Discrectionary SPDR Fund   \n",
    "                           'XLF',  # XLF Financial SPDR Fund  \n",
    "                           'XLK',  # XLK Technology SPDR Fund  \n",
    "                           'XLE',  # XLE Energy SPDR Fund  \n",
    "                           'XLV',  # XLV Health Care SPRD Fund  \n",
    "                           'XLI',  # XLI Industrial SPDR Fund  \n",
    "                           'XLP',  # XLP Consumer Staples SPDR Fund   \n",
    "                           'XLB',  # XLB Materials SPDR Fund  \n",
    "                           'XLU')  # XLU Utilities SPRD Fund\n",
    "    \n",
    "    context.historical_bars = 100\n",
    "    context.feature_window = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the different imports required in this program. SVC, LinearSVC, NuSVC, and RandomClassifier are classifiers that will be used in the program for learning. We also import for preprocessing for normalizing data, a counter for counting occurences, and Numpy for numerical analysis.\n",
    "\n",
    "In initialize(), we assign the stock universe to context.stocks, the number of bars to assign as history to context.historical_bars, and the number of features to include in each feature set to context.feature_window. \n",
    "\n",
    "Now, lets move to the handle_data() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_data(context, data):\n",
    "    prices = history(bar_count = context.historical_bars, frequency='1d', field='price')\n",
    "\n",
    "    for stock in context.stocks:\n",
    "        try:\n",
    "            # Simple grab of the short and long moving average\n",
    "            ma1 = data[stock].mavg(50)\n",
    "            ma2 = data[stock].mavg(200)\n",
    "\n",
    "            start_bar = context.feature_window\n",
    "            price_list = prices[stock].tolist()\n",
    "            \n",
    "            # X holds the feature sets and y holds the labels.\n",
    "            X = []\n",
    "            y = []\n",
    "\n",
    "            bar = start_bar\n",
    "\n",
    "            # feature creation\n",
    "            while bar < len(price_list)-1:\n",
    "                try:\n",
    "                    end_price = price_list[bar+1]\n",
    "                    begin_price = price_list[bar]\n",
    "\n",
    "                    pricing_list = []\n",
    "                    xx = 0\n",
    "                    for _ in range(context.feature_window):\n",
    "                        price = price_list[bar-(context.feature_window-xx)]\n",
    "                        pricing_list.append(price)\n",
    "                        xx += 1\n",
    "\n",
    "                    features = np.around(np.diff(pricing_list) / pricing_list[:-1] * 100.0, 1)\n",
    "\n",
    "\n",
    "                    #print(features)\n",
    "\n",
    "                    # Classify current feature sets according to relative values of end_price and begin_price\n",
    "                    if end_price > begin_price:\n",
    "                        label = 1\n",
    "                    else:\n",
    "                        label = -1\n",
    "\n",
    "                    bar += 1\n",
    "                    X.append(features)\n",
    "                    y.append(label)\n",
    "\n",
    "                except Exception as e:\n",
    "                    bar += 1\n",
    "                    print(('feature creation',str(e)))\n",
    "\n",
    "            #Create classifier\n",
    "            clf = RandomForestClassifier()\n",
    "\n",
    "            #Grab current feature set, normalize, and test label prediction with classifier\n",
    "            last_prices = price_list[-context.feature_window:]\n",
    "            current_features = np.around(np.diff(last_prices) / last_prices[:-1] * 100.0, 1)\n",
    "\n",
    "            # Append current feature to container of all feature sets, then use preprocessing to convert data to a range b/w -1 \n",
    "            # and 1. It's a common standardization technique of machine learning.\n",
    "            X.append(current_features)\n",
    "            X = preprocessing.scale(X)\n",
    "\n",
    "            # Separate data, where current_features is the current feature set, and X is the set of feature sets with known\n",
    "            # labels.\n",
    "            current_features = X[-1]\n",
    "            X = X[:-1]\n",
    "\n",
    "            # We now train the classifier with fit() function, and then perform prediction on current feature set\n",
    "            clf.fit(X,y)\n",
    "            p = clf.predict(current_features)[0]\n",
    "\n",
    "            print(('Prediction',p))\n",
    "            # To test our performance, we pass to order_target_percent\n",
    "            if p == 1:\n",
    "                order_target_percent(stock,0.11)\n",
    "            elif p == -1:\n",
    "                order_target_percent(stock,-0.11)            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "            \n",
    "    record('ma1',ma1)\n",
    "    record('ma2',ma2)\n",
    "    record('Leverage',context.account.leverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to take place in the handle_data() function is the grabbing of historical daily pricing for the stock universe defined in initialize(). In the for loop, we iterate through each stock in our stock universe. We first perform short and long term moving average. \n",
    "\n",
    "Then going into prepping for the classifier (in this case, the Random Forest classifier), we create variable X for holding array of feature sets, and variable y for holding the associated labels for each feature set in X. The while loop begins our feature creation. end_price and begin_price are assigned the next day and current day prices, respectively. These are used later for assigning label to feature set created later in the while loop. In the nested for loop, we populate our current feature list (defined as pricing_list), and then we normalize it to percent change outside the for loop with numpy and assign it to features variable. And then finally, we associate a label with the current feature set based on the relative values of the end_price and begin_price variables. \n",
    "\n",
    "Then outside the while loop, we begin the set up of our classifier, followed by feeding it the training feature sets and their associated labels. After training, we then feed it a test feature set, and perform prediction of its label. After completion of prediction, we then test performance of our prediction. \n",
    "\n",
    "The plot shows that performance for this classifier was not good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we include our calculations of the moving averages, we get different performance results (shown below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if p == 1 and ma1 > ma2:\n",
    "        order_target_percent(stock,0.11)\n",
    "    elif p == -1 and ma1 < ma2:\n",
    "        order_target_percent(stock,-0.11) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare to using just the moving averages..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if ma1 > ma2:\n",
    "        order_target_percent(stock,0.11)\n",
    "    elif ma1 < ma2:\n",
    "        order_target_percent(stock,-0.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the random forest classifier performs 1% better on our returns and has a Sharpe ratio that is greater by 0.5. This doesn't show much of significant improvement compared to using moving averages.\n",
    "\n",
    "However, we can perform prediction with multiple classifiers. One example includes multiple classifiers that are in agreement with each other. Another example is the mode of the prediction of multiple classifiers. We'll attempt the former."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            clf1 = RandomForestClassifier()\n",
    "            clf2 = LinearSVC()\n",
    "            clf3 = NuSVC()\n",
    "            clf4 = LogisticRegression()\n",
    "\n",
    "            last_prices = price_list[-context.feature_window:]\n",
    "            current_features = np.around(np.diff(last_prices) / last_prices[:-1] * 100.0, 1)\n",
    "\n",
    "            X.append(current_features)\n",
    "            X = preprocessing.scale(X)\n",
    "\n",
    "            current_features = X[-1]\n",
    "            X = X[:-1]\n",
    "\n",
    "            clf1.fit(X,y)\n",
    "            clf2.fit(X,y)\n",
    "            clf3.fit(X,y)\n",
    "            clf4.fit(X,y)\n",
    "\n",
    "            p1 = clf1.predict(current_features)[0]\n",
    "            p2 = clf2.predict(current_features)[0]\n",
    "            p3 = clf3.predict(current_features)[0]\n",
    "            p4 = clf4.predict(current_features)[0]\n",
    "            \n",
    "            \n",
    "            if Counter([p1,p2,p3,p4]).most_common(1)[0][1] >= 4:\n",
    "                p = Counter([p1,p2,p3,p4]).most_common(1)[0][0]\n",
    "                \n",
    "            else:\n",
    "                p = 0\n",
    "                \n",
    "            print(('Prediction',p))\n",
    "\n",
    "\n",
    "            if p == 1 and ma1 > ma2:\n",
    "                order_target_percent(stock,0.11)\n",
    "            elif p == -1 and ma1 < ma2:\n",
    "                order_target_percent(stock,-0.11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four classifiers have been added: RandomForestClassifier, LinearSVC, NuSVC, and LogisticRegression. With these four classifiers, we see improvement in our performance (shown below). We have a 3.8% increase in return and 0.75 increase in the Sharpe ratio in comparison to the performance with just moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
